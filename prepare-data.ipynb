{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06bf4cb8",
   "metadata": {},
   "source": [
    "### Bengali Automatic Speech Recognition\n",
    "\n",
    "- wav2vec2 https://huggingface.co/docs/transformers/model_doc/wav2vec2\n",
    "\n",
    "    - Wav2Vec2 is a speech model that accepts a float array corresponding to the raw waveform of the speech signal.   \n",
    "    - Wav2Vec2 model was trained using connectionist temporal classification (CTC) so the model output has to be decoded using Wav2Vec2CTCTokenizer.  \n",
    "    - It is trained on 16 kHz sampling rate\n",
    "    \n",
    "\n",
    "- whisper https://huggingface.co/docs/transformers/model_doc/whisper\n",
    "    - trained on more data than wav2vec2 (680,000 hours of labelled data)\n",
    "    -  Transformer based encoder-decoder model, also referred to as a sequence-to-sequence model. \n",
    "    - It maps a sequence of audio spectrogram features to a sequence of text tokens. First, the raw audio inputs are converted to a log-Mel spectrogram by action of the feature extractor. The Transformer encoder then encodes the spectrogram to form a sequence of encoder hidden states. \n",
    "    - Finally, the decoder autoregressively predicts text tokens, conditional on both the previous tokens and the encoder hidden states\n",
    "\n",
    "\n",
    "A few methods implemented:\n",
    "\n",
    "- ASR with language model\n",
    "- boosting with ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "387a3785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from glob import glob\n",
    "from transformers import AutoFeatureExtractor, pipeline\n",
    "import pandas as pd\n",
    "import IPython\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import Dataset,DatasetDict\n",
    "import torch\n",
    "import gc\n",
    "import wave\n",
    "from scipy.io import wavfile\n",
    "import scipy.signal as sps\n",
    "from datasets import Audio\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "tqdm.pandas()\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from glob import glob\n",
    "import IPython.display as ipd\n",
    "\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4285dd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the train data\n",
    "train = pd.read_csv(\"data/train.csv\")\n",
    "train.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd50e95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
