{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62d00bc3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-30 01:37:40.637986: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-30 01:37:41.515936: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-30 01:37:43,140] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from transformers import Trainer\n",
    "from datasets import load_metric, Audio\n",
    "from transformers import (\n",
    "    Wav2Vec2ForCTC,\n",
    "    Wav2Vec2Processor,\n",
    "    Wav2Vec2CTCTokenizer,\n",
    "    Wav2Vec2FeatureExtractor\n",
    ") \n",
    "import re\n",
    "import librosa\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "from transformers.trainer_utils import get_last_checkpoint, is_main_process\n",
    "from transformers.utils import check_min_version, send_example_telemetry\n",
    "from transformers.utils.versions import require_version\n",
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b534fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BengaliDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.processor = Wav2Vec2Processor(\n",
    "    feature_extractor=feature_extractor, \n",
    "    tokenizer=tokenizer)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # First read and pre-process the audio file\n",
    "        audio = self.read_audio(self.data.loc[idx,'path'])\n",
    "        audio = processor(\n",
    "            audio, \n",
    "            sampling_rate=16000\n",
    "        ).input_values[0]\n",
    "        \n",
    "        with processor.as_target_processor():\n",
    "            labels = processor(self.data.loc[idx,'sentence']).input_ids\n",
    "            \n",
    "        return {'input_values': audio, 'labels': labels}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def read_audio(self, mp3_path):\n",
    "        target_sr = 16000  # Set the target sampling rate\n",
    "        \n",
    "        audio, sr = librosa.load(mp3_path, sr=target_sr)  # Load with original sampling rate\n",
    "        \n",
    "        \n",
    "        return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cb05bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vocab(dataframe):\n",
    "    \"\"\"\n",
    "    Saves the processed vocab file as 'vocab.json', to be ingested by tokenizer\n",
    "    \"\"\"\n",
    "    vocab = construct_vocab(dataframe['sentence'].tolist())\n",
    "    vocab_dict = {v: k for k, v in enumerate(vocab)}\n",
    "    vocab_dict[\"__\"] = vocab_dict[\" \"]\n",
    "    _ = vocab_dict.pop(\" \")\n",
    "    vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
    "    vocab_dict[\"[PAD]\"] = len(vocab_dict)\n",
    "\n",
    "    with open('vocab.json', 'w') as fl:\n",
    "        json.dump(vocab_dict, fl)\n",
    "\n",
    "\n",
    "def ctc_data_collator(batch):\n",
    "    \"\"\"\n",
    "    data collator function to dynamically pad the data\n",
    "    \"\"\"\n",
    "    input_features = [{\"input_values\": sample[\"input_values\"]} for sample in batch]\n",
    "    label_features = [{\"input_ids\": sample[\"labels\"]} for sample in batch]\n",
    "    batch = processor.pad(\n",
    "        input_features,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    with processor.as_target_processor():\n",
    "        labels_batch = processor.pad(\n",
    "            label_features,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "    labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "    batch[\"labels\"] = labels\n",
    "    return batch\n",
    "\n",
    "def construct_vocab(texts):\n",
    "    \"\"\"\n",
    "    Get unique characters from all the text in a list\n",
    "    \"\"\"\n",
    "    all_text = \" \".join(texts)\n",
    "    vocab = list(set(all_text))\n",
    "    return vocab\n",
    "    \n",
    "### Data cleaning, remove punctuations and lowercase\n",
    "def remove_special_characters(string):\n",
    "\n",
    "    chars_to_ignore_regex = ', ? . ! - \\; \\: \\\" “ % ” �'\n",
    "    \n",
    "    clean = re.sub(chars_to_ignore_regex, \"\", string).lower() + \" \"\n",
    "  \n",
    "    return clean\n",
    "\n",
    "\n",
    "# ### Word Error Rate (Evaluation Metrics)\n",
    "# def compute_metrics(pred):\n",
    "\n",
    "#     wer_metric = load_metric(\"wer\")\n",
    "\n",
    "#     pred_logits = pred.predictions\n",
    "#     pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "#     pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "#     pred_str = processor.batch_decode(pred_ids)\n",
    "#     # we do not want to group tokens when computing the metrics\n",
    "#     label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "#     wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "#     return {\"wer\": wer}\n",
    "wer_metric = load_metric(\"wer\")\n",
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1968a0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on samples: 9340, Validation on samples: 296\n"
     ]
    }
   ],
   "source": [
    "output_dir = './mms'\n",
    "model_name = 'facebook/mms-300m'\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"/home/ubuntu/bengali/data/train.csv\")\n",
    "\n",
    "df['sentence'] = df['sentence'].apply(lambda x: remove_special_characters(x))\n",
    "\n",
    "# Add the full path to the audio files\n",
    "df['path'] = df['id'].apply(lambda x: os.path.join('/home/ubuntu/bengali/data/train_mp3s', x+'.mp3'))\n",
    "\n",
    "# Filter the dataset\n",
    "train = df[df['split'] == 'train'].sample(frac=0.01, random_state=10).reset_index(drop=True)\n",
    "val = df[df['split'] == 'valid'].sample(frac=0.01, random_state=200).reset_index(drop=True)\n",
    "\n",
    "print(f\"Training on samples: {len(train)}, Validation on samples: {len(val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24709313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9340"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "371c6b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>split</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000005f3362c</td>\n",
       "      <td>ও বলেছে আপনার ঠিকানা!</td>\n",
       "      <td>train</td>\n",
       "      <td>/home/ubuntu/bengali/data/train_mp3s/000005f33...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00001dddd002</td>\n",
       "      <td>কোন মহান রাষ্ট্রের নাগরিক হতে চাও?</td>\n",
       "      <td>train</td>\n",
       "      <td>/home/ubuntu/bengali/data/train_mp3s/00001dddd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00001e0bc131</td>\n",
       "      <td>আমি তোমার কষ্টটা বুঝছি, কিন্তু এটা সঠিক পথ না।</td>\n",
       "      <td>train</td>\n",
       "      <td>/home/ubuntu/bengali/data/train_mp3s/00001e0bc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000024b3d810</td>\n",
       "      <td>নাচ শেষ হওয়ার পর সকলে শরীর ধুয়ে একসঙ্গে ভোজন...</td>\n",
       "      <td>train</td>\n",
       "      <td>/home/ubuntu/bengali/data/train_mp3s/000024b3d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000028220ab3</td>\n",
       "      <td>হুমম, ওহ হেই, দেখো।</td>\n",
       "      <td>train</td>\n",
       "      <td>/home/ubuntu/bengali/data/train_mp3s/000028220...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                           sentence  split  \\\n",
       "0  000005f3362c                             ও বলেছে আপনার ঠিকানা!   train   \n",
       "1  00001dddd002                কোন মহান রাষ্ট্রের নাগরিক হতে চাও?   train   \n",
       "2  00001e0bc131    আমি তোমার কষ্টটা বুঝছি, কিন্তু এটা সঠিক পথ না।   train   \n",
       "3  000024b3d810  নাচ শেষ হওয়ার পর সকলে শরীর ধুয়ে একসঙ্গে ভোজন...  train   \n",
       "4  000028220ab3                               হুমম, ওহ হেই, দেখো।   train   \n",
       "\n",
       "                                                path  \n",
       "0  /home/ubuntu/bengali/data/train_mp3s/000005f33...  \n",
       "1  /home/ubuntu/bengali/data/train_mp3s/00001dddd...  \n",
       "2  /home/ubuntu/bengali/data/train_mp3s/00001e0bc...  \n",
       "3  /home/ubuntu/bengali/data/train_mp3s/000024b3d...  \n",
       "4  /home/ubuntu/bengali/data/train_mp3s/000028220...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c7aa6b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>split</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93c612a8caa2</td>\n",
       "      <td>তোমরা নিশ্চয়ই এখানে নতুন, তাই না?</td>\n",
       "      <td>train</td>\n",
       "      <td>/home/ubuntu/bengali/data/train_mp3s/93c612a8c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eea39689fa84</td>\n",
       "      <td>দুই লেগের প্লে-অফের প্রথম ম্যাচে পর্তুগাল নিজে...</td>\n",
       "      <td>train</td>\n",
       "      <td>/home/ubuntu/bengali/data/train_mp3s/eea39689f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2c3dabcc8c82</td>\n",
       "      <td>ধারণা করা হচ্ছে, অতিরিক্ত মদ্যপান করার কারণে প...</td>\n",
       "      <td>train</td>\n",
       "      <td>/home/ubuntu/bengali/data/train_mp3s/2c3dabcc8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4e1af4af2707</td>\n",
       "      <td>দুপুরে সেখানে যাওয়ার পর ইমন নানাভাবে কালক্ষেপণ...</td>\n",
       "      <td>train</td>\n",
       "      <td>/home/ubuntu/bengali/data/train_mp3s/4e1af4af2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ebe5597e64ed</td>\n",
       "      <td>তিনি বেণীমাধবকে সাথে নিয়ে উত্তরের পথে গমন করেন।</td>\n",
       "      <td>train</td>\n",
       "      <td>/home/ubuntu/bengali/data/train_mp3s/ebe5597e6...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                           sentence  split  \\\n",
       "0  93c612a8caa2                 তোমরা নিশ্চয়ই এখানে নতুন, তাই না?   train   \n",
       "1  eea39689fa84  দুই লেগের প্লে-অফের প্রথম ম্যাচে পর্তুগাল নিজে...  train   \n",
       "2  2c3dabcc8c82  ধারণা করা হচ্ছে, অতিরিক্ত মদ্যপান করার কারণে প...  train   \n",
       "3  4e1af4af2707  দুপুরে সেখানে যাওয়ার পর ইমন নানাভাবে কালক্ষেপণ...  train   \n",
       "4  ebe5597e64ed  তিনি বেণীমাধবকে সাথে নিয়ে উত্তরের পথে গমন করেন।   train   \n",
       "\n",
       "                                                path  \n",
       "0  /home/ubuntu/bengali/data/train_mp3s/93c612a8c...  \n",
       "1  /home/ubuntu/bengali/data/train_mp3s/eea39689f...  \n",
       "2  /home/ubuntu/bengali/data/train_mp3s/2c3dabcc8...  \n",
       "3  /home/ubuntu/bengali/data/train_mp3s/4e1af4af2...  \n",
       "4  /home/ubuntu/bengali/data/train_mp3s/ebe5597e6...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d4952ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>split</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c64d5c2936c4</td>\n",
       "      <td>আব্দুল হাকিম মুক্তিযুদ্ধের সংগঠক ছিলেন।</td>\n",
       "      <td>valid</td>\n",
       "      <td>/home/ubuntu/bengali/data/train_mp3s/c64d5c293...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fa923f59c89c</td>\n",
       "      <td>তিনি অনুশীলন সমিতি কতিপয় সদস্যের সাথে তার বন্...</td>\n",
       "      <td>valid</td>\n",
       "      <td>/home/ubuntu/bengali/data/train_mp3s/fa923f59c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9b69fbe6db68</td>\n",
       "      <td>চলচ্চিত্রটি পরিচালনা করেছেন অভিনেতা রাঘব লরেন্স।</td>\n",
       "      <td>valid</td>\n",
       "      <td>/home/ubuntu/bengali/data/train_mp3s/9b69fbe6d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f4abf5b8fbf3</td>\n",
       "      <td>এটি এখন পর্যন্ত ফুটবলের সর্বোচ্চ সংস্থা ফিফার ...</td>\n",
       "      <td>valid</td>\n",
       "      <td>/home/ubuntu/bengali/data/train_mp3s/f4abf5b8f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2041d3effe22</td>\n",
       "      <td>বাংলা ও ইংরেজি ছাড়া উনি হিন্দি, ওড়িয়া, অসমী...</td>\n",
       "      <td>valid</td>\n",
       "      <td>/home/ubuntu/bengali/data/train_mp3s/2041d3eff...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                           sentence  split  \\\n",
       "0  c64d5c2936c4           আব্দুল হাকিম মুক্তিযুদ্ধের সংগঠক ছিলেন।   valid   \n",
       "1  fa923f59c89c  তিনি অনুশীলন সমিতি কতিপয় সদস্যের সাথে তার বন্...  valid   \n",
       "2  9b69fbe6db68  চলচ্চিত্রটি পরিচালনা করেছেন অভিনেতা রাঘব লরেন্স।   valid   \n",
       "3  f4abf5b8fbf3  এটি এখন পর্যন্ত ফুটবলের সর্বোচ্চ সংস্থা ফিফার ...  valid   \n",
       "4  2041d3effe22  বাংলা ও ইংরেজি ছাড়া উনি হিন্দি, ওড়িয়া, অসমী...  valid   \n",
       "\n",
       "                                                path  \n",
       "0  /home/ubuntu/bengali/data/train_mp3s/c64d5c293...  \n",
       "1  /home/ubuntu/bengali/data/train_mp3s/fa923f59c...  \n",
       "2  /home/ubuntu/bengali/data/train_mp3s/9b69fbe6d...  \n",
       "3  /home/ubuntu/bengali/data/train_mp3s/f4abf5b8f...  \n",
       "4  /home/ubuntu/bengali/data/train_mp3s/2041d3eff...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c47d8f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_vocab(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0eef5cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/mms-300m and are newly initialized: ['lm_head.weight', 'lm_head.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Wav2Vec2CTCTokenizer(\n",
    "    \"vocab.json\", \n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    word_delimiter_token=\"__\"\n",
    ")\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(\n",
    "    feature_size=1, \n",
    "    sampling_rate=16000, \n",
    "    padding_value=0.0, \n",
    "    do_normalize=True, \n",
    "    return_attention_mask=False\n",
    ")\n",
    "processor = Wav2Vec2Processor(\n",
    "    feature_extractor=feature_extractor, \n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    model_name,\n",
    "    ctc_loss_reduction=\"mean\", \n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    vocab_size = len(tokenizer),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd15d251",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = BengaliDataset(train)\n",
    "valid_ds = BengaliDataset(val)\n",
    "\n",
    "\n",
    "\n",
    "model.freeze_feature_encoder()\n",
    "model.to('cuda')\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=5,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        num_train_epochs=10,\n",
    "        gradient_checkpointing=True,\n",
    "        fp16=True,\n",
    "        save_steps=100,\n",
    "        eval_steps=100,\n",
    "        logging_steps=10,\n",
    "        learning_rate=2e-5,\n",
    "        warmup_steps=0,\n",
    "        save_total_limit=2,\n",
    "        do_eval = False)\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=ctc_data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_ds,   \n",
    "    eval_dataset=valid_ds,    \n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdf8fc0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9340"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e7065c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mariffnzhn\u001b[0m (\u001b[33mmalaysia-ai\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/bengali/aisyah/training/wandb/run-20230830_013809-rbsysh1i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/malaysia-ai/huggingface/runs/rbsysh1i' target=\"_blank\">summer-frog-21</a></strong> to <a href='https://wandb.ai/malaysia-ai/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/malaysia-ai/huggingface' target=\"_blank\">https://wandb.ai/malaysia-ai/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/malaysia-ai/huggingface/runs/rbsysh1i' target=\"_blank\">https://wandb.ai/malaysia-ai/huggingface/runs/rbsysh1i</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1554\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1560\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:1815\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1812\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1814\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1815\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   1816\u001b[0m     total_batched_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1817\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rng_to_sync:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/data_loader.py:384\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 384\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer_utils.py:707\u001b[0m, in \u001b[0;36mRemoveColumnsCollator.__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: List[\u001b[38;5;28mdict\u001b[39m]):\n\u001b[1;32m    706\u001b[0m     features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_remove_columns(feature) \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m features]\n\u001b[0;32m--> 707\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_collator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 20\u001b[0m, in \u001b[0;36mctc_data_collator\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mctc_data_collator\u001b[39m(batch):\n\u001b[1;32m     17\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m    data collator function to dynamically pad the data\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m     input_features \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_values\u001b[39m\u001b[38;5;124m\"\u001b[39m: sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_values\u001b[39m\u001b[38;5;124m\"\u001b[39m]} \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[1;32m     21\u001b[0m     label_features \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]} \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[1;32m     22\u001b[0m     batch \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m     23\u001b[0m         input_features,\n\u001b[1;32m     24\u001b[0m         padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     25\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     26\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[3], line 20\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mctc_data_collator\u001b[39m(batch):\n\u001b[1;32m     17\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m    data collator function to dynamically pad the data\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m     input_features \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_values\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_values\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m} \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[1;32m     21\u001b[0m     label_features \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]} \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[1;32m     22\u001b[0m     batch \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m     23\u001b[0m         input_features,\n\u001b[1;32m     24\u001b[0m         padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     25\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     26\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5e8bd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
