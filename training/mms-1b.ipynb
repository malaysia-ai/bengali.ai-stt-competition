{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19757191",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-30 01:15:47.681662: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-30 01:15:48.543044: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from transformers import Trainer\n",
    "from datasets import load_metric, Audio\n",
    "from transformers import (\n",
    "    Wav2Vec2ForCTC,\n",
    "    Wav2Vec2Processor,\n",
    "    Wav2Vec2CTCTokenizer,\n",
    "    Wav2Vec2FeatureExtractor\n",
    ") \n",
    "import re\n",
    "import librosa\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "from transformers.trainer_utils import get_last_checkpoint, is_main_process\n",
    "from transformers.utils import check_min_version, send_example_telemetry\n",
    "from transformers.utils.versions import require_version\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "\n",
    "wer_metric = load_metric(\"wer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0b49755",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BengaliDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.processor = Wav2Vec2Processor(\n",
    "    feature_extractor=feature_extractor, \n",
    "    tokenizer=tokenizer)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # First read and pre-process the audio file\n",
    "        audio = self.read_audio(self.df.loc[idx]['path'])\n",
    "        audio = processor(\n",
    "            audio, \n",
    "            sampling_rate=16000\n",
    "        ).input_values[0]\n",
    "        \n",
    "        with processor.as_target_processor():\n",
    "            labels = processor(self.df.loc[idx]['sentence']).input_ids\n",
    "        return {'input_values': audio, 'labels': labels}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def read_audio(self, mp3_path):\n",
    "        target_sr = 16000  # Set the target sampling rate\n",
    "        \n",
    "        audio, sr = librosa.load(mp3_path, sr=None) \n",
    "        audio_array = librosa.resample(audio, orig_sr=sr, target_sr=target_sr)\n",
    "        \n",
    "        return audio_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1acbf052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vocab(dataframe):\n",
    "    \"\"\"\n",
    "    Saves the processed vocab file as 'vocab.json', to be ingested by tokenizer\n",
    "    \"\"\"\n",
    "    vocab = construct_vocab(dataframe['sentence'].tolist())\n",
    "    vocab_dict = {v: k for k, v in enumerate(vocab)}\n",
    "    vocab_dict[\"__\"] = vocab_dict[\" \"]\n",
    "    _ = vocab_dict.pop(\" \")\n",
    "    vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
    "    vocab_dict[\"[PAD]\"] = len(vocab_dict)\n",
    "    \n",
    "    print(vocab_dict)\n",
    "    \n",
    "    new_vocab_dict = {\"ben\": vocab_dict}\n",
    "\n",
    "\n",
    "    with open('vocab.json', 'w') as fl:\n",
    "        json.dump(new_vocab_dict, fl)\n",
    "\n",
    "\n",
    "def ctc_data_collator(batch):\n",
    "    \"\"\"\n",
    "    data collator function to dynamically pad the data\n",
    "    \"\"\"\n",
    "    input_features = [{\"input_values\": sample[\"input_values\"]} for sample in batch]\n",
    "    label_features = [{\"input_ids\": sample[\"labels\"]} for sample in batch]\n",
    "    batch = processor.pad(\n",
    "        input_features,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    with processor.as_target_processor():\n",
    "        labels_batch = processor.pad(\n",
    "            label_features,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "    labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "    batch[\"labels\"] = labels\n",
    "    return batch\n",
    "\n",
    "def construct_vocab(texts):\n",
    "    \"\"\"\n",
    "    Get unique characters from all the text in a list\n",
    "    \"\"\"\n",
    "    all_text = \" \".join(texts)\n",
    "    vocab = list(set(all_text))\n",
    "    return vocab\n",
    "    \n",
    "### Data cleaning, remove punctuations and lowercase\n",
    "def remove_special_characters(string):\n",
    "\n",
    "    chars_to_ignore_regex = ', ? . ! - \\; \\: \\\" “ % ” �'\n",
    "    \n",
    "    clean_text = re.sub(chars_to_ignore_regex, \"\", string).lower() + \" \"\n",
    "  \n",
    "    return clean_text\n",
    "\n",
    "\n",
    "### Word Error Rate (Evaluation Metrics)\n",
    "def compute_metrics(pred):\n",
    "\n",
    "\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c11a027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on samples: 747, Validation on samples: 30\n"
     ]
    }
   ],
   "source": [
    "output_dir = './checkpoint-mms'\n",
    "model_name = 'facebook/mms-1b-all'\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"/home/ubuntu/bengali/data/train.csv\")\n",
    "\n",
    "df['sentence'] = df['sentence'].apply(lambda x: remove_special_characters(x))\n",
    "\n",
    "# Add the full path to the audio files\n",
    "df['path'] = df['id'].apply(lambda x: os.path.join('/home/ubuntu/bengali/data/train_mp3s', x+'.mp3'))\n",
    "\n",
    "# Filter the dataset\n",
    "train = df[df['split'] == 'train'].sample(frac=0.0008, random_state=10).reset_index(drop=True)\n",
    "val = df[df['split'] == 'valid'].sample(frac=0.001, random_state=10).reset_index(drop=True)\n",
    "\n",
    "print(f\"Training on samples: {len(train)}, Validation on samples: {len(val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88bc5d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'‘': 0, ';': 1, '।': 2, 'ধ': 3, '!': 4, 'গ': 5, 'ঝ': 6, 'ঐ': 7, 'আ': 8, 'ঃ': 9, 'ফ': 10, '\"': 11, 'ু': 12, 'ো': 13, 'ম': 14, 'স': 15, '-': 16, 'ে': 17, 'ষ': 18, '/': 19, 'ঢ়': 20, 'ঠ': 21, 'ঊ': 22, ':': 23, 'ন': 24, 'ড়': 25, 'ও': 26, 'ং': 27, 'ঙ': 28, 'ী': 29, '”': 30, 'ত': 31, '্': 32, 'ঞ': 33, '.': 34, 'ৰ': 35, 'উ': 36, 'া': 37, 'ণ': 38, 'ব': 39, 'থ': 40, 'ভ': 41, 'ূ': 42, 'ঈ': 43, \"'\": 44, 'ঢ': 45, 'ছ': 46, 'ই': 47, 'ৎ': 48, '“': 49, 'শ': 51, 'জ': 52, '॥': 53, 'ট': 54, 'য': 55, 'ল': 56, 'ড': 57, 'ি': 58, 'দ': 59, 'ক': 60, '–': 61, 'এ': 62, 'হ': 63, 'ৗ': 64, 'ৌ': 65, '—': 66, '‚': 67, 'র': 68, 'ঘ': 69, '়': 70, ',': 71, '৵': 72, 'ঋ': 73, '’': 74, '…': 75, 'ৈ': 76, 'ৃ': 77, 'খ': 78, '৷': 79, 'প': 80, 'য়': 81, 'অ': 82, 'ঁ': 83, 'চ': 84, 'ঔ': 85, '?': 86, '__': 50, '[UNK]': 87, '[PAD]': 88}\n"
     ]
    }
   ],
   "source": [
    "save_vocab(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5321a43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2ForCTC, AutoProcessor\n",
    "import torch\n",
    "\n",
    "# processor = AutoProcessor.from_pretrained(model_name)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(model_name)\n",
    "\n",
    "tokenizer = Wav2Vec2CTCTokenizer(\n",
    "    \"vocab.json\", \n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    word_delimiter_token=\"__\",\n",
    "    target_lang = \"ben\"\n",
    ")\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(\n",
    "    feature_size=1, \n",
    "    sampling_rate=16000, \n",
    "    padding_value=0.0, \n",
    "    do_normalize=True, \n",
    "    return_attention_mask=False\n",
    ")\n",
    "processor = Wav2Vec2Processor(\n",
    "    feature_extractor=feature_extractor, \n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# model = Wav2Vec2ForCTC.from_pretrained(\n",
    "#     model_name,\n",
    "#     ctc_loss_reduction=\"mean\", \n",
    "#     pad_token_id=processor.tokenizer.pad_token_id,\n",
    "#     vocab_size = len(tokenizer),\n",
    "# )\n",
    "processor.tokenizer.set_target_lang(\"ben\")\n",
    "model.load_adapter(\"ben\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c51cb76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab.json', 'w') as fopen:\n",
    "    json.dump(processor.tokenizer.vocab['ben'], fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f61049",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = BengaliDataset(train)\n",
    "valid_ds = BengaliDataset(val)\n",
    "\n",
    "\n",
    "model.freeze_feature_encoder()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        group_by_length=True,\n",
    "        per_device_train_batch_size=1,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        num_train_epochs=1,\n",
    "        gradient_checkpointing=True,\n",
    "        fp16=True,\n",
    "        save_steps=100,\n",
    "        eval_steps=100,\n",
    "        logging_steps=10,\n",
    "        learning_rate=3e-4,\n",
    "        warmup_steps=500,\n",
    "        save_total_limit=2,\n",
    "        do_eval=False\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=ctc_data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_ds,   \n",
    "    eval_dataset=valid_ds,    \n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d0efb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34d4bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 run_speech_recognition_ctc.py \\\n",
    "\t--model_name_or_path=\"facebook/mms-1b-all\" \\\n",
    "\t--output_dir=\"./mms-1b\" \\\n",
    "\t--num_train_epochs=\"1\" \\\n",
    "\t--per_device_train_batch_size=\"5\" \\\n",
    "\t--learning_rate=\"2e-5\" \\\n",
    "\t--evaluation_strategy=\"steps\" \\\n",
    "\t--save_steps=\"400\" \\\n",
    "\t--eval_steps=\"100\" \\\n",
    "\t--layerdrop=\"0.0\" \\\n",
    "\t--save_total_limit=\"2\" \\\n",
    "\t--gradient_checkpointing \\\n",
    "\t--fp16 \\\n",
    "    --do_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647a0a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "~/.local/bin/deepspeed run_speech_recognition.py --deepspeed ds_config_zero3.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2edc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "~/.local/bin/deepspeed run_speech_recognition_ctc.py \\\n",
    "--deepspeed ds_config_zero3.json \\\n",
    "--model_name_or_path=\"./mms-1b-all\" \\\n",
    "\t--output_dir=\"./mms-1b\" \\\n",
    "\t--num_train_epochs=\"1\" \\\n",
    "\t--per_device_train_batch_size=\"16\" \\\n",
    "\t--learning_rate=\"2e-5\" \\\n",
    "\t--evaluation_strategy=\"steps\" \\\n",
    "\t--save_steps=\"400\" \\\n",
    "\t--eval_steps=\"400\" \\\n",
    "    --logging_steps=\"50\"\\\n",
    "\t--layerdrop=\"0.0\" \\\n",
    "\t--save_total_limit=\"2\" \\\n",
    "\t--gradient_checkpointing \\\n",
    "--do_train \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
