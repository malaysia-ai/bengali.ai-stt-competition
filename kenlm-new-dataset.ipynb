{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8aaecf1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>path</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>এক মাইলের কম থেকে শুরু করে বেশি দূরত্ব যা কিনা...</td>\n",
       "      <td>/home/ubuntu/bengali/data/train/10007728875760...</td>\n",
       "      <td>/home/ubuntu/bengali/data/train/10007728875760...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ব্রাজিলের জাতীয় কংগ্রেস সামাজিক বিবাহকে 10 বছর...</td>\n",
       "      <td>/home/ubuntu/bengali/data/train/10027935128938...</td>\n",
       "      <td>/home/ubuntu/bengali/data/train/10027935128938...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>স্থলরেখা থেকে অনেক দূরে থাকায় মার্কিন যুক্তরাষ...</td>\n",
       "      <td>/home/ubuntu/bengali/data/train/10031075386419...</td>\n",
       "      <td>/home/ubuntu/bengali/data/train/10031075386419...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>বর্তমানে অনেক সামি আধুনিক ব্যবসায় কাজ করে।সামি...</td>\n",
       "      <td>/home/ubuntu/bengali/data/train/10032118353482...</td>\n",
       "      <td>/home/ubuntu/bengali/data/train/10032118353482...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>অশ্বরোহীর পাদান তার পায়ের সাপোর্টের জন্য যা অশ...</td>\n",
       "      <td>/home/ubuntu/bengali/data/train/10033964478741...</td>\n",
       "      <td>/home/ubuntu/bengali/data/train/10033964478741...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  \\\n",
       "0  এক মাইলের কম থেকে শুরু করে বেশি দূরত্ব যা কিনা...   \n",
       "1  ব্রাজিলের জাতীয় কংগ্রেস সামাজিক বিবাহকে 10 বছর...   \n",
       "2  স্থলরেখা থেকে অনেক দূরে থাকায় মার্কিন যুক্তরাষ...   \n",
       "3  বর্তমানে অনেক সামি আধুনিক ব্যবসায় কাজ করে।সামি...   \n",
       "4  অশ্বরোহীর পাদান তার পায়ের সাপোর্টের জন্য যা অশ...   \n",
       "\n",
       "                                                path  \\\n",
       "0  /home/ubuntu/bengali/data/train/10007728875760...   \n",
       "1  /home/ubuntu/bengali/data/train/10027935128938...   \n",
       "2  /home/ubuntu/bengali/data/train/10031075386419...   \n",
       "3  /home/ubuntu/bengali/data/train/10032118353482...   \n",
       "4  /home/ubuntu/bengali/data/train/10033964478741...   \n",
       "\n",
       "                                            filename  \n",
       "0  /home/ubuntu/bengali/data/train/10007728875760...  \n",
       "1  /home/ubuntu/bengali/data/train/10027935128938...  \n",
       "2  /home/ubuntu/bengali/data/train/10031075386419...  \n",
       "3  /home/ubuntu/bengali/data/train/10032118353482...  \n",
       "4  /home/ubuntu/bengali/data/train/10033964478741...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet('/home/ubuntu/Raijin/LATEST_DATA_WAV2VEC2.parquet')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3fcbfa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text.txt', 'w') as fopen:\n",
    "    fopen.write('\\n'.join(df['sentence'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "116a2967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /home/ubuntu/bengali/speech-to-text/text.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 8195431 types 196542\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:2358504 2:18123259904 3:33981116416 4:54369783808 5:79289270272\n",
      "Statistics:\n",
      "1 196542 D1=0.701262 D2=0.963045 D3+=1.21669\n",
      "2 1743729/1899206 D1=0.802367 D2=1.16807 D3+=1.32639\n",
      "3 2835867/3131703 D1=0.83147 D2=1.42044 D3+=1.54664\n",
      "4 2927430/3250095 D1=0.805353 D2=1.50683 D3+=2.22252\n",
      "5 2638681/2936409 D1=0.0653422 D2=1.9611 D3+=2.96163\n",
      "Memory estimate for binary LM:\n",
      "type     MB\n",
      "probing 221 assuming -p 1.5\n",
      "probing 265 assuming -r models -p 1.5\n",
      "trie    112 without quantization\n",
      "trie     62 assuming -q 8 -b 8 quantization \n",
      "trie     98 assuming -a 22 array pointer compression\n",
      "trie     49 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:2358504 2:27899664 3:56717340 4:70258320 5:73883068\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "################################################################################*********###########\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:2358504 2:27899664 3:56717340 4:70258320 5:73883068\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:181583544 kB\tVmRSS:62840 kB\tRSSMax:32142524 kB\tuser:9.61962\tsys:5.14201\tCPU:14.7616\treal:12.664\n",
      "Reading out.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Identifying n-grams omitted by SRI\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Quantizing\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Writing trie\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!kenlm/build/bin/lmplz --text text.txt --arpa out.arpa -o 5 --prune 0 1 1\n",
    "!kenlm/build/bin/build_binary -q 8 -b 7 -a 256 trie out.arpa out.trie.klm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f841945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 855M\r\n",
      "-rwxrwxrwx  1 ubuntu ubuntu  55K Sep 15 16:46  Inference-Trial-0.ipynb\r\n",
      "-rwxrwxrwx  1 ubuntu ubuntu 2.2K Sep  3 15:55  README.md\r\n",
      "-rw-r--r--  1 ubuntu ubuntu  915 Sep  7 11:31  audio-augmentation.ipynb\r\n",
      "-rwxrwxrwx  1 ubuntu ubuntu 9.5M Sep 15 15:32  bengali-speech-eda.ipynb\r\n",
      "-rwxrwxrwx  1 ubuntu ubuntu 767K Sep 10 08:41  check-unknown.ipynb\r\n",
      "-rw-r--r--  1 ubuntu ubuntu  24K Sep 13 10:12  create-fold.ipynb\r\n",
      "-rwxrwxrwx  1 ubuntu ubuntu  13K Sep 10 02:29  explore-vocab.ipynb\r\n",
      "-rw-r--r--  1 ubuntu ubuntu 7.6K Sep  6 04:01  inference-ariff.ipynb\r\n",
      "-rwxrwxrwx  1 ubuntu ubuntu  32K Sep  3 15:55  inference-validation.ipynb\r\n",
      "-rwxrwxrwx  1 ubuntu ubuntu  10K Sep  3 15:55  inference.ipynb\r\n",
      "-rwxrwxrwx  1 ubuntu ubuntu  245 Sep 16 03:19  install-kenlm.sh\r\n",
      "drwxr-xr-x  9 ubuntu ubuntu 4.0K Feb 11  2022  kenlm\r\n",
      "drwxr-xr-x  3 ubuntu ubuntu 4.0K Sep 13 00:11  kenlm-4gram\r\n",
      "drwxr-xr-x  2 ubuntu ubuntu 4.0K Sep 11 08:37  kenlm-arpa\r\n",
      "-rw-r--r--  1 ubuntu ubuntu 133M Sep 16 03:03  kenlm-deprecated-text.txt\r\n",
      "-rw-r--r--  1 ubuntu ubuntu 6.5K Sep 16 03:21  kenlm-new-dataset.ipynb\r\n",
      "-rwxrwxrwx  1 ubuntu ubuntu 2.4K Sep 11 07:37  kenlm-test.ipynb\r\n",
      "-rwxrwxrwx  1 ubuntu ubuntu  35K Sep 16 03:15  kenlm.ipynb\r\n",
      "-rw-r--r--  1 ubuntu ubuntu  513 Sep 12 05:25  merged-ensemble.py\r\n",
      "-rw-r--r--  1 ubuntu ubuntu 521M Sep 16 03:21  out.arpa\r\n",
      "-rw-r--r--  1 ubuntu ubuntu  49M Sep 16 03:22  out.trie.klm\r\n",
      "-rwxrwxrwx  1 ubuntu ubuntu 3.3K Sep  3 15:55  prepare-data.ipynb\r\n",
      "-rw-r--r--  1 ubuntu ubuntu 133M Sep 16 03:12  text.txt\r\n",
      "-rw-r--r--  1 ubuntu ubuntu 9.9M Sep 16 02:48 'the problem of overfitting butter chicken.ipynb'\r\n",
      "drwxrwxrwx 13 ubuntu ubuntu 4.0K Sep 16 00:20  training\r\n",
      "-rwxrwxrwx  1 ubuntu ubuntu 1.1K Sep  4 10:30  vocab.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f1fe2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\data\\\r\n",
      "ngram 1=196542\r\n",
      "ngram 2=1743729\r\n",
      "ngram 3=2835867\r\n",
      "ngram 4=2927430\r\n",
      "\r\n",
      "\\1-grams:\r\n",
      "-6.3277507\t<unk>\t0\r\n",
      "0\t<s>\t-1.2403845\r\n",
      "-1.7006247\t</s>\t0\r\n",
      "-2.8978767\tএক\t-0.37472534\r\n",
      "-5.34024\tমাইলের\t-0.14626579\r\n",
      "-3.6946073\tকম\t-0.24526523\r\n",
      "-2.604384\tথেকে\t-0.43365178\r\n",
      "-3.132855\tশুরু\t-1.0808644\r\n",
      "-2.7338002\tকরে\t-0.45755637\r\n",
      "-3.2481034\tবেশি\t-0.32089615\r\n",
      "-4.302331\tদূরত্ব\t-0.19532079\r\n",
      "-3.1718116\tযা\t-0.3034509\r\n",
      "-3.9696317\tকিনা\t-0.36083865\r\n"
     ]
    }
   ],
   "source": [
    "!head -20 ./kenlm-4gram-newdataset/out.arpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "979c662e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('/home/ubuntu/Raijin/full_vocab.json') as fopen:\n",
    "    vocab = json.load(fopen)\n",
    "    vocab = {v: k for k, v in vocab.items()}\n",
    "    \n",
    "vocab = [vocab[i] for i in range(len(vocab))]\n",
    "vocab\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "341793db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the LM will be faster if you build a binary file.\n",
      "Reading /home/ubuntu/bengali/speech-to-text/kenlm-4gram-newdataset/out.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "import kenlm\n",
    "from pyctcdecode import build_ctcdecoder\n",
    "\n",
    "lm = './kenlm-4gram-newdataset/out.arpa'\n",
    "kenlm_model = lm\n",
    "decoder = build_ctcdecoder(\n",
    "    vocab,\n",
    "    kenlm_model,\n",
    "    alpha=0.5,\n",
    "    beta=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "217b3a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6b0243",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    Wav2Vec2ForCTC,\n",
    "    Wav2Vec2Processor,\n",
    "    Wav2Vec2CTCTokenizer,\n",
    "    Wav2Vec2FeatureExtractor\n",
    ") \n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8db6691",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from typing import Mapping, Tuple\n",
    "import librosa\n",
    "# import en_core_web_sm\n",
    "\n",
    "\n",
    "import librosa\n",
    "\n",
    "class BengaliDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, processor):\n",
    "        self.df = df\n",
    "        self.processor = processor\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.df.loc[idx]['path']\n",
    "        audio_array = self.read_audio(audio_path)\n",
    "        \n",
    "        inputs = self.processor(\n",
    "            audio_array,\n",
    "            sampling_rate=16000,\n",
    "            return_tensors='pt'  \n",
    "        )\n",
    "        \n",
    "        with self.processor.as_target_processor():\n",
    "            labels = self.processor(self.df.loc[idx]['sentence']).input_ids\n",
    "        \n",
    "        return {'input_values': inputs['input_values'][0], 'labels': labels}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def read_audio(self, mp3_path):\n",
    "        target_sr = 16000  # Set the target sampling rate\n",
    "        \n",
    "        audio, sr = librosa.load(mp3_path, sr=None)  # Load with original sampling rate\n",
    "        audio_array = librosa.resample(audio, orig_sr=sr, target_sr=target_sr)\n",
    "        \n",
    "        return audio_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4ac614",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"/home/ubuntu/bengali/aisyah/training/mms-1b/checkpoint-16400\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"/home/ubuntu/bengali/aisyah/training/mms-1b/checkpoint-16400\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444f1afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.02, random_state=42)\n",
    "\n",
    "test = test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a78e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "test_ds = BengaliDataset(test, processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ab0773",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    input_values = [item['input_values'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "\n",
    "    input_values_padded = pad_sequence(input_values, batch_first=True, padding_value=0.0)\n",
    "\n",
    "    labels = [torch.tensor(label) for label in labels]\n",
    "\n",
    "    return {'input_values': input_values_padded, 'labels': labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb94eaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(\n",
    "    test_ds, batch_size=batch_size, shuffle=False, num_workers=2, collate_fn=custom_collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c12ace5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cuda')\n",
    "model.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdde17aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader):\n",
    "        x = batch[\"input_values\"]\n",
    "        x = x.to(\"cuda\", non_blocking=True)\n",
    "        with torch.cuda.amp.autocast(True):\n",
    "            y = model(x).logits\n",
    "        y = y.detach().cpu().numpy()\n",
    "\n",
    "        for l in y:\n",
    "            out = decoder.decode_beams(l,prune_history = True)\n",
    "            d_lm, lm_state, timesteps, logit_score, lm_score = out[0]\n",
    "            sentences.append(d_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f49688c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jiwer\n",
    "\n",
    "jiwerresult_0 = []\n",
    "\n",
    "for x in range(len(sentences)):\n",
    "\n",
    "    jiwer_ = jiwer.wer(sentences[x],test.iloc[x,1])\n",
    "\n",
    "    jiwerresult_0.append(jiwer_)\n",
    "    \n",
    "    \n",
    "avg_wer_0 = sum(jiwerresult_0)/len(sentences)\n",
    "print(avg_wer_0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
